{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Merging, Cleaning Data Sets and Adding Variables#\n",
    "\n",
    "This notebook will be used to explore merging the data sets, cleaning the data sets (changing various values to np.NaN, converting fields to the appropriate value type), and adding calculated variables. This code will be repeated in the exploratory and modeling notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cross = pd.read_csv('000_Cross-Sectional.csv', low_memory=False)\n",
    "base = pd.read_csv('00_Baseline.csv', low_memory=False)\n",
    "visit1 = pd.read_csv('01_Visit1.csv', low_memory=False)\n",
    "visit2 = pd.read_csv('02_Visit2.csv', low_memory=False)\n",
    "visit3 = pd.read_csv('03_Visit3.csv', low_memory=False)\n",
    "visit4 = pd.read_csv('04_Visit4.csv', low_memory=False)\n",
    "visit5 = pd.read_csv('05_Visit5.csv', low_memory=False)\n",
    "visit6 = pd.read_csv('06_Visit6.csv', low_memory=False)\n",
    "visit7 = pd.read_csv('07_Visit7.csv', low_memory=False)\n",
    "visit8 = pd.read_csv('08_Visit8.csv', low_memory=False)\n",
    "visit9 = pd.read_csv('09_Visit9.csv', low_memory=False)\n",
    "visit10 = pd.read_csv('10_Visit10.csv', low_memory=False)\n",
    "\n",
    "pd.set_option('display.max_rows', 120)\n",
    "cross.rename(columns={'ID':'SWANID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Merging Data Sets##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.merge(cross, base)\n",
    "data = pd.merge(data, visit1, on='SWANID', how='outer')\n",
    "data = pd.merge(data, visit2, on='SWANID', how='outer')\n",
    "data = pd.merge(data, visit3, on='SWANID', how='outer')\n",
    "data = pd.merge(data, visit4, on='SWANID', how='outer')\n",
    "data = pd.merge(data, visit5, on='SWANID', how='outer')\n",
    "data = pd.merge(data, visit6, on='SWANID', how='outer')\n",
    "data = pd.merge(data, visit7, on='SWANID', how='outer')\n",
    "data = pd.merge(data, visit8, on='SWANID', how='outer')\n",
    "data = pd.merge(data, visit9, on='SWANID', how='outer')\n",
    "data = pd.merge(data, visit10, on='SWANID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3302, 9214)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cleaning Data Sets##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing coded values for missing/null data to np.NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.replace(' ', np.nan, inplace=True)\n",
    "data.replace('-9', np.nan, inplace=True)\n",
    "data.replace('-1', np.nan, inplace=True)\n",
    "data.replace('-7', np.nan, inplace=True)\n",
    "data.replace('-8', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correcting data types**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discrimination scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[['COURTES0', 'RESPECT0', 'POORSER0', 'NOTSMAR0', 'AFRAIDO0', 'DISHONS0', 'BETTER0', 'INSULTE0', 'HARASSE0', 'IGNORED0']] = data[['COURTES0', 'RESPECT0', 'POORSER0', 'NOTSMAR0', 'AFRAIDO0', 'DISHONS0', 'BETTER0', 'INSULTE0', 'HARASSE0', 'IGNORED0']].astype(float)\n",
    "data[['COURTES1', 'RESPECT1', 'POORSER1', 'NOTSMAR1', 'AFRAIDO1', 'DISHONS1', 'BETTER1', 'INSULTE1', 'HARASSE1', 'IGNORED1']] = data[['COURTES1', 'RESPECT1', 'POORSER1', 'NOTSMAR1', 'AFRAIDO1', 'DISHONS1', 'BETTER1', 'INSULTE1', 'HARASSE1', 'IGNORED1']].astype(float)\n",
    "data[['COURTES2', 'RESPECT2', 'POORSER2', 'NOTSMAR2', 'AFRAIDO2', 'DISHONS2', 'BETTER2', 'INSULTE2', 'HARASSE2', 'IGNORED2']] = data[['COURTES2', 'RESPECT2', 'POORSER2', 'NOTSMAR2', 'AFRAIDO2', 'DISHONS2', 'BETTER2', 'INSULTE2', 'HARASSE2', 'IGNORED2']].astype(float)\n",
    "data[['COURTES3', 'RESPECT3', 'POORSER3', 'NOTSMAR3', 'AFRAIDO3', 'DISHONS3', 'BETTER3', 'INSULTE3', 'HARASSE3', 'IGNORED3']] = data[['COURTES3', 'RESPECT3', 'POORSER3', 'NOTSMAR3', 'AFRAIDO3', 'DISHONS3', 'BETTER3', 'INSULTE3', 'HARASSE3', 'IGNORED3']].astype(float)\n",
    "data[['COURTES7', 'RESPECT7', 'POORSER7', 'NOTSMAR7', 'AFRAIDO7', 'DISHONS7', 'BETTER7', 'INSULTE7', 'HARASSE7', 'IGNORED7']] = data[['COURTES7', 'RESPECT7', 'POORSER7', 'NOTSMAR7', 'AFRAIDO7', 'DISHONS7', 'BETTER7', 'INSULTE7', 'HARASSE7', 'IGNORED7']].astype(float)\n",
    "data[['COURTES10', 'RESPECT10', 'POORSER10', 'NOTSMAR10', 'AFRAIDO10', 'DISHONS10', 'BETTER10', 'INSULTE10', 'HARASSE10', 'IGNORED10']] = data[['COURTES10', 'RESPECT10', 'POORSER10', 'NOTSMAR10', 'AFRAIDO10', 'DISHONS10', 'BETTER10', 'INSULTE10', 'HARASSE10', 'IGNORED10']].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[['AGE0', 'AGE1', 'AGE2', 'AGE3', 'AGE4', 'AGE5', 'AGE6', 'AGE7', 'AGE8', 'AGE9', 'AGE10']] = data[['AGE0', 'AGE1', 'AGE2', 'AGE3', 'AGE4', 'AGE5', 'AGE6', 'AGE7', 'AGE8', 'AGE9', 'AGE10']].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Calculated Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Average Discrimination Score*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[:, 'DISC_SCORE0'] = 5 - data[['COURTES0', 'RESPECT0', 'POORSER0', 'NOTSMAR0', 'AFRAIDO0', 'DISHONS0', \n",
    "                                   'BETTER0', 'INSULTE0', 'HARASSE0', 'IGNORED0']].mean(axis=1)\n",
    "data.loc[:, 'DISC_SCORE1'] = 5 - data[['COURTES1', 'RESPECT1', 'POORSER1', 'NOTSMAR1', 'AFRAIDO1', 'DISHONS1', \n",
    "                                   'BETTER1', 'INSULTE1', 'HARASSE1', 'IGNORED1']].mean(axis=1)\n",
    "data.loc[:, 'DISC_SCORE2'] = 5 - data[['COURTES2', 'RESPECT2', 'POORSER2', 'NOTSMAR2', 'AFRAIDO2', 'DISHONS2', \n",
    "                                    'BETTER2', 'INSULTE2', 'HARASSE2', 'IGNORED2']].mean(axis=1)\n",
    "data.loc[:, 'DISC_SCORE3'] = 5 - data[['COURTES3', 'RESPECT3', 'POORSER3', 'NOTSMAR3', 'AFRAIDO3', 'DISHONS3', \n",
    "                                    'BETTER3', 'INSULTE3', 'HARASSE3', 'IGNORED3']].mean(axis=1)\n",
    "data.loc[:, 'DISC_SCORE7'] = 5 - data[['COURTES7', 'RESPECT7', 'POORSER7', 'NOTSMAR7', 'AFRAIDO7', 'DISHONS7', \n",
    "                                    'BETTER7', 'INSULTE7', 'HARASSE7', 'IGNORED7']].mean(axis=1)\n",
    "data.loc[:, 'DISC_SCORE10'] = 5 - data[['COURTES10', 'RESPECT10', 'POORSER10', 'NOTSMAR10', 'AFRAIDO10', 'DISHONS10', \n",
    "                                    'BETTER10', 'INSULTE10', 'HARASSE10', 'IGNORED10']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reason for Discrimination*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor x in conversion:\\n    if(data[x].dtype == np.float64):\\n        convert_bin_fl(x)\\n    else:\\n        convert_bin(x)\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_binary(cols):\n",
    "    data[cols] = data[cols].map({'1':0, '2':1})\n",
    "    data[cols].replace(np.nan, 0, inplace=True)\n",
    "    print data[cols].value_counts(dropna=False)\n",
    "\n",
    "def convert_binary_float(cols):\n",
    "    data[cols] = data[cols].map({1:0, 2:1})\n",
    "    data[cols].replace(np.nan, 0, inplace=True) # this may need to come out for certain variables\n",
    "    print data[cols].value_counts(dropna=False)\n",
    "    \n",
    "\"\"\"\n",
    "for x in \"array_of_column_names\":\n",
    "    if(data[x].dtype == np.float64):\n",
    "        convert_bin_fl(x)\n",
    "    else:\n",
    "        convert_bin(x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['RACE_REASON0'] = data.MAINREA0.map({'1':1, '2':1, '3':0, '4':0, '5':0, '6':0, '7':0, '8':0, '9':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
